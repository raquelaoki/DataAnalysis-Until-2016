{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOI6GSpFwj1sHjpY29qaMGE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raquelaoki/DataAnalysis2016/blob/master/RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DREA5JOddRKt",
        "colab_type": "text"
      },
      "source": [
        "##RNN\n",
        "\n",
        "These are a type of Neural Networks (NN) that can have a long or shor 'memory'. In a scenario where the sequence of interest has $K$ elements/time stemps, the NN will have $K$ layers, one for each element. Considering a folded RNN format, there are 3 nodes: $x_t$ (input at step $t$), $s_t$ (hidden state at time $t$, the 'memory', $s_t = f(U\\times x_t+W\\times s_{t-1}))$ and $o_t$ (the output on the state $t$, $o_t = actfunc(V\\times s_t)$). A RNN share the parameters $U$, $V$ and $W$ across all steps.\n",
        "\n",
        "There are some variations that can be adopted: one-to-many (Image Description), many-to-one (sentiment analysis), many-to-many (Text Generation, Translation). \n",
        "\n",
        "###LSTM \n",
        "\n",
        "Long Short Term Memory (LSTM) are a subtype of RNN, design for long-term dependences. While RNN can also support long-term dependences, on its pure form, has a poor performance and struggle to connect past information. A LSTM cell has gates, that let the information go through or holds information, usually a sigmoid neural net. A LSTM has 3 of these gates: \n",
        "1. Forget gate layer: considering $h_{t-1}$ and $x_t$, and outputs a values between 0 and 1 for each number in the cell state $C_{t-1}$. Values near to 1 means that the values are kept, and 0 that it should get rid of it. $f_t = \\sigma(W_f.[h_{t-1},x_t]+b_f)$\n",
        "2. Decide if the information will be store in the cell. We do this in two parts: first, we use a input gate layer to decide which values to update. Then, a new layer calculate the new candidate values. \n",
        "$i_t=\\sigma(W_i.[h_{t-1},x_t]+b_i)$ and $\\hat{C}_t = tann(W_c.[h_{t-1},x_t]+b_c)$\n",
        "3. The cell is update by combine the values previous calculated: $C_t=f_t*C_{t-1}+i_t*\\hat{C_t}$. The output will be $o_t=\\sigma(W_o [h_{t-1}, x_t]+b_o)$ and the new hidden state $h_t = o_t*tanh(C_t)$.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "References:\n",
        "- RNN \n",
        " - [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
        " - [RNN and Python Example](https://towardsdatascience.com/recurrent-neural-networks-by-example-in-python-ffd204f99470)\n",
        " - [Tutorial in lua 1](https://github.com/jcjohnson/torch-rnn)\n",
        " - [Tutorial in lua 1](https://github.com/karpathy/char-rnn/blob/master/model/LSTM.lua)\n",
        "\n",
        "- LSTM \n",
        " - [Understanding LSTM](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) \n",
        " - [Illustrated Guide to LSTM’s and GRU’s: A step by step explanation](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21)\n",
        "  - [Fall of RNN/LSTM](https://towardsdatascience.com/the-fall-of-rnn-lstm-2d1594c74ce0)\n",
        "  - [How to Develop LSTM Models for Time Series Forecasting](https://machinelearningmastery.com/how-to-develop-lstm-models-for-time-series-forecasting/)\n",
        " - [Tutorial for LSTM - Tensorflow 1](https://adventuresinmachinelearning.com/recurrent-neural-networks-lstm-tutorial-tensorflow/)\n",
        " - [Tutorial for RNN/LSTM - Tensorflow 2](https://github.com/dragen1860/TensorFlow-2.x-Tutorials/blob/master/09-RNN-Sentiment-Analysis/main.py)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Note: Tensorflow 2\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gRqqTpyOALsU",
        "colab": {}
      },
      "source": [
        "#!pip uninstall tensorflow -y\n",
        "#!pip uninstall tf-nightly -y\n",
        "#!pip uninstall tf-nightly-gpu -y \n",
        "#!pip install tensorflow-gpu==2.0.0 \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIfwJgauc5Bm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import  os\n",
        "import  tensorflow as tf\n",
        "import  numpy as np\n",
        "from    tensorflow import keras\n",
        "\n",
        "#Using the code from available in:\n",
        "#https://github.com/dragen1860/TensorFlow-2.x-Tutorials/blob/master/09-RNN-Sentiment-Analysis/main.py\n",
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
        "tf.__version__"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qScCINeFwF_M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#what does os.environ? \n",
        "#fix random seed for reproducibility and checking tensor version\n",
        "tf.random.set_seed(22)\n",
        "np.random.seed(22)\n",
        "assert tf.__version__.startswith('2.')\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRY2zrRl0R6O",
        "colab_type": "code",
        "outputId": "47c12808-1e1b-4d0f-c489-b2ebb88fbb8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "#TEST WITH A DIFFERENT DATASET\n",
        "#while its a functional exemple, does not show the generated text back \n",
        "\n",
        "\n",
        "\n",
        "# load the dataset but only keep the top n words, zero the rest\n",
        "top_words = 10000\n",
        "# truncate and pad input sequences\n",
        "max_review_length = 80\n",
        "(X_train, y_train), (X_test, y_test) = keras.datasets.imdb.load_data(num_words=top_words)\n",
        "# X_train = tf.convert_to_tensor(X_train)\n",
        "# y_train = tf.one_hot(y_train, depth=2)\n",
        "print('Pad sequences (samples x time)')\n",
        "x_train = keras.preprocessing.sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
        "x_test = keras.preprocessing.sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pad sequences (samples x time)\n",
            "x_train shape: (25000, 80)\n",
            "x_test shape: (25000, 80)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpXlD_W22Hwo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNN(keras.Model):\n",
        "\n",
        "    def __init__(self, units, num_classes, num_layers):\n",
        "        super(RNN, self).__init__()\n",
        "\n",
        "        # self.cells = [keras.layers.LSTMCell(units) for _ in range(num_layers)]\n",
        "        #\n",
        "        # self.rnn = keras.layers.RNN(self.cells, unroll=True)\n",
        "        self.rnn = keras.layers.LSTM(units, return_sequences=True)\n",
        "        self.rnn2 = keras.layers.LSTM(units)\n",
        "        # self.cells = (keras.layers.LSTMCell(units) for _ in range(num_layers))\n",
        "        # #\n",
        "        # self.rnn = keras.layers.RNN(self.cells, return_sequences=True, return_state=True)\n",
        "        # self.rnn = keras.layers.LSTM(units, unroll=True)\n",
        "        # self.rnn = keras.layers.StackedRNNCells(self.cells)\n",
        "\n",
        "        # have 1000 words totally, every word will be embedding into 100 length vector\n",
        "        # the max sentence lenght is 80 words, top_words = 10000\n",
        "        self.embedding = keras.layers.Embedding(top_words, 100, input_length=max_review_length)\n",
        "        self.fc = keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, inputs, training=None, mask=None):\n",
        "\n",
        "        # print('x', inputs.shape)\n",
        "        # [b, sentence len] => [b, sentence len, word embedding]\n",
        "        x = self.embedding(inputs)\n",
        "        # print('embedding', x.shape)\n",
        "        x = self.rnn(x) \n",
        "        x = self.rnn2(x) \n",
        "        # print('rnn', x.shape)\n",
        "\n",
        "        x = self.fc(x)\n",
        "        print(x.shape)\n",
        "\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smGlQ8fE4Vwi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#def main():\n",
        "\n",
        "units = 64\n",
        "num_classes = 2\n",
        "batch_size = 32\n",
        "epochs = 20\n",
        "\n",
        "model = RNN(units, num_classes, num_layers=2)\n",
        "\n",
        "model.compile(optimizer=keras.optimizers.Adam(0.001),\n",
        "              loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# train\n",
        "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs,\n",
        "          validation_data=(x_test, y_test), verbose=1)\n",
        "\n",
        "# evaluate on test set\n",
        "scores = model.evaluate(x_test, y_test, batch_size, verbose=1)\n",
        "print(\"Final test loss and accuracy :\", scores)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}